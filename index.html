
<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
	<link rel="shortcut icon" type="image/png" href="burger.png">
		<title>BigMAC: Big Model Adaptation for Computer vision - ICCV 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <meta property='og:title' content="🍔BigMAC: Big Model Adaptation for Computer vision - ICCV 2023"/>
	</head>
	<body>
		<!-- Wrapper -->
        <div id = "wrapper">
            <!-- Header-->
            <header id="header" class = "alt" style = "width: 100%">
                <!-- <div class = "bgimg" style = "height: 475px">-->
                <div class = "bgimg" style = "height: 250px; width: 100%">
                  <div style = "height: 20px; width: 100%"></div>
                  <h1 style = "text-shadow: 0.1px 0.1px #000000">BigMAC: Big Model Adaptation for Computer vision</h1>
                  <h2 style = "text-shadow: 0.1px 0.1px #000000">ICCV 2023 - October 2nd, Paris Convention Center</h2>
                </div>
            </header>

            <!-- Nav -->
            <nav id="nav">
                <ul>
                    <li><a href="#intro" class="active">Introduction</a></li>
                    <li><a href="#format">Format</a></li>
<!--                    <li><a href="#cfp">Call for Papers</a></li>-->
                    <li><a href="#schedule">Schedule</a></li>
                    <li><a href="#speakers">Speakers</a></li>
                    <li><a href="#info">Organizers</a></li>
                </ul>
            </nav>

            <!-- Main -->
            <div id="main">
                <!-- Introduction -->
                <section id="intro" class="main">
                    <header class="major" style="text-align:center">

                        <iframe width="560" height="315" src="https://www.youtube.com/embed/XiouM3MEOKs?si=yPdHxck9N38xIg3c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

                        <h2>Summary</h2>
                    </header>
                    <div class="spotlight" style="width: 80%; margin: auto;">
                        <div class="content">
                            <p>
                                Computer vision has continued to grow with unprecedented progress in recent years.
				When comparing this to Natural Language Processing (NLP), one thing seems clear: the size of neural networks will keep growing, as will the capabilities of these models will too.
				Nonetheless, as the size and complexity of these models continue to expand, adapting them to novel tasks and domains presents significant challenges – that differ from those faced in the NLP community. 
				    
                            </p>
                            <p>
                                The goal of this workshop is to explore and discuss ways of dealing with the new reality of ever larger models in computer vision.
                                The sheer parameter and training dataset sizes mean that these models often cannot be trained by academia and some models might not even fit on large GPUs for inference.
                                These developments not only bring new challenges for computer vision researchers and practitioners but also many novel opportunities.
                                In this workshop, we aim to bring together researchers from academia and industry to talk and discuss topics that are increasingly of importance for the vision community.
                            </p>
                            <p>
                                The workshop BigMAC: Big Model Adaptation for Computer Vision, will cover topics related to how large pretrained models can be effectively used:
                                <ul style="padding-left:40px;">
                                    <li>Prompting methods and techniques for vision models</li>
                                    <li>New methodologies for  fine-tuning pretrained models</li>
                                    <li>Leveraging multi-modal weak-supervision techniques</li>
                                    <li>Scaling and fine-tuning with self-supervision </li>
                                    <li>Finetuning large general-pretrained models robustness</li>
                                    <li>Quantization and efficiency</li>
                                </ul>
                            </p>
                            <p>
                                This is the first iteration of the BigMAC event and does NOT have a call for papers, which might be added in future iterations. The workshop is organized as a half-day event with oral talks from the invited speakers.
                            </p>
                        </div>
                    </div>
                </section>
                <section id="format" class="main">
                    <header class="major" style="text-align:center">
                        <h2>Format</h2>
                    </header>
                    <div class="spotlight" style="width: 80%; margin: auto;">
                        <div class="content">
                            <p>
                                The workshop is a half-day event. It will consist of a series of invited talks on this topic from the leading experts in academia and industry.
                            </p>
                        </div>
                    </div>
                </section>

                <section id="schedule" class="main special">
                    <header class="major" style="text-align:center">
                        <h2>Schedule</h2>
                    </header>
                    <p>The workshop will take place on October 2nd, 9am-1pm, in the Paris Convention Center, Room S03. Times are Paris time.</p>
                    <p>Recording available <a href = "https://youtu.be/XiouM3MEOKs">here</a></p>

                    <div class="table-wrapper" style="width:100%">
                        <table class="alt">
                            <tbody>
                                <col width="20%">
                                <col width="25%">
                                <col width="60%">
                                
                                <tr>
                                    <td>9:00-9:15</td>
                                    <td>Welcome (Yuki M. Asano)</td>
                                    <td>
                                        <button type="button" class="collapsible"><a href="pdfs/bigmac_iccv.pdf">slides</a></button>
                                        <div class="collapsible_content"><p>Scale has arrived in vision, stand-alone and via Visual Language Models. What to do?</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>9:15-9:45</td>
                                    <td>Neil Houlsby</td>
                                    <td>
                                        <button type="button" class="collapsible">Advances in Visual Pretraining for LLMs</button>
                                        <div class="collapsible_content"><p> LLMs are increasingly being augmented with visual capabilities. This progress is, in part, driven by improvements in scalable and general visual pre-training. In this talk, I will present some recent advances in visual pre-training with a focus on Vision Transformers, and the results of applying such models in VLMs. These advances tackle scaling ViTs to unprecedented sizes, endowing them with the ability to flexibly handle any images, and unlocking the efficacy of sparsity.</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>9:45-10:15</td>
                                    <td>Maria Attarian</td>
                                    <td>
                                        <button type="button" class="collapsible">Large models as world representations for robotic downstream tasks</button>
                                        <div class="collapsible_content"><p>The goal of the field of robotics is to create physical systems that can operate dexterously in dynamic real-world environments. This requires being able to handle ambiguous task descriptions, and creating action plans which take into account the physical constraints of the embodiment and environment. Large pretrained multimodal models contain a wealth of world knowledge, which makes them a promising backbone for such robotic systems. However despite their strengths, these models don’t understand physical dynamics, and do not natively produce robotic control outputs. In this talk we describe these strengths and weaknesses in detail, and discuss how to make these models more suitable for robotic planning and control.</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>10:15-10:45</td>
                                    <td>Samir Gadre</td>
                                    <td>
                                        <button type="button" class="collapsible">Robust fine-tuning of zero-shot models</button>
                                        <div class="collapsible_content"><p>TBD</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>10:45-11:00</td>
                                    <td></td>
                                    <td>Break</td>
                                </tr>
                                
                                <tr>
                                    <td>11:00-11:30</td>
                                    <td>Ishan Misra</td>
                                    <td>
                                        <button type="button" class="collapsible">SSL helps multimodal pretraining scale to more modalities and data <a href="pdfs/Ishan.pdf">(slides)</a>
                                        </button>
                                        <div class="collapsible_content"><p>In this talk, I’ll show how self-supervised learning can be used to improve foundational multimodal models to scale to more modalities, learn better representations, and be more efficient.
                                            A big challenge when training foundational multimodal models is the scarcity of paired data. While there is copious amounts of (image, text) data, other modalities such as depth or IMU have limited dataset sizes overall. Our first work in this direction called ImageBind shows that images can be used as a universal signal to “bind” multiple different modalities. We show that naturally co-occurring image pairings such as (image, IMU), (image, depth) can be automatically used to learn a share embedding space where unseen pairs of modalities are aligned. ImageBind enables emergent zero-shot recognition, cross-modal retrieval and generation.
                                            Our second work shows that using self-supervised learning as a “pre” pre-training stage improves multimodal (image, text) representations across a wide range of model sizes and data sizes. Pre pre-training improves the performance of foundational models with billions of parameters trained on billions of images while also speeding up their convergence. The resulting models show state-of-the-art performance for full finetuning, linear probing, zero-shot recognition tasks in image and video domains.
                                            </p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>11:30-12:00</td>
                                    <td>Aditi Raghunathan</td>
                                    <td>
                                        <button type="button" class="collapsible">Rethinking fine-tuning to mitigate feature distortion  <a href="pdfs/Aditi.pdf">(slides)</a></button>
                                        <div class="collapsible_content"><p>Pretrained models provide strong feature representations which can be adapted to downstream tasks via fine-tuning. However, in this talk, we show that standard fine-tuning procedures can cause feature distortion: correspondences between in-distribution and out-of-distribution features are weakened. As a result, fine-tuned models are not “maximally” robust to distribution shifts. How do we devise better fine-tuning procedures that minimize such distortion? Via theoretical constructions, we provide simple, scalable and effective modifications to the fine-tuning process that vastly improve the accuracy and robustness of fine-tuned models. Overall, this talk highlights the importance of various factors often overlooked in the fine-tuning process in effectively preserving pretrained knowledge.</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>12:00-12:30</td>
                                    <td>Sayak Paul</td>
                                    <td>
                                        <button type="button" class="collapsible">Controlling Text-to-Image Diffusion Models  <a href="pdfs/Sayak.pdf">(slides)</a></button>
                                        <div class="collapsible_content"><p>Large-scale text-to-image diffusion models like DALL-E 2, Imagen, and Stable Diffusion have been quite successful at the task of text-to-image generation. However, their text-only conditional control offers limited flexibility and controllability to the end users. To elevate that degree of freedom, we need ways to condition the generation process better. This talk will discuss some of the most promising and effective approaches to controlling text-to-image diffusion models. The approaches will be a mix of both training-time and inference-time techniques.</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>12:30-1:00</td>
                                    <td>Carl Vondrick</td>
                                    <td>
                                        <button type="button" class="collapsible">Visual Intelligence via Code Synthesis and Execution</button>
                                        <div class="collapsible_content"><p>Computer vision algorithms need to combine many skills — spatial, physical, mathematical, geometrical, and cognitive — in order to accurately analyze the visual world. In this talk, I will show how code synthesis equips neural networks with these skills, thereby providing versatile representations for answering questions and recognizing objects. Through a series of experimental results, I will moreover show how this approach naturally provides inherent explainability of the decision making process, while also achieving state-of-the-art zero-shot performance across different tasks and benchmarks.</p></div>
                                    </td>
                                </tr>
                                
                                <tr>
                                    <td>1:00</td>
                                    <td>Closing remarks</td>
                                    <td></td>
                                </tr>
                                
                            </tbody>
                        </table>
                    </div>                    
                </section>

                <section id="speakers" class="main special">
                    <header class="major" style="text-align:center">
                        <h2>Speakers</h2>
                    </header>
                    <div class="table-wrapper" style ="width:100%">
                        <table cellpadding = "0" cellspacing = "0">
                            <tr>
                                <td><img class="rounded-img-dark" height="125px" src="photos/houlsby.jpeg"><br><a href="https://neilhoulsby.github.io/">Neil Houlsby<br>Google Brain</a></td>
                                <td><img class="rounded-img-dark" height="125px" src="photos/attarian.jpg"><br><a href="https://jmattarian.com/">Maria Attarian<br>Google Brain, University of Toronto</a></td>
                                <td><img class="rounded-img-dark" height="125px" src="photos/samir.jpeg"><br><a href="https://sagadre.github.io/">Samir Gadre<br>University of Columbia</a></td>
                            </tr><tr>
                                <td><img class="rounded-img-dark" height="125px" src="photos/misra.jpg"><br><a href="https://imisra.github.io/">Ishan Misra<br>FAIR</a></td> 
                                <td><img class="rounded-img-dark" height="125px" src="photos/raghunathan.png"><br><a href="https://www.cs.cmu.edu/~aditirag/">Aditi Raghunathan<br>Carnegie Mellon University</a></td>
                                <td><img class="rounded-img-dark" height="125px" src="photos/sayak.jpeg"><br><a href="https://sayak.dev/about/">Sayak Paul<br>HuggingFace</td>
                                <td><img class="rounded-img-dark" height="125px" src="photos/vondrick.jpg"><br><a href="http://www.cs.columbia.edu/~vondrick/">Carl Vondrick<br>Columbia University</td>
                            </tr>
                        </table>
                    </div>
                </section>
						
                <section id="info" class="main special">
                    <header class="major">
                        <h2>Organizers</h2>
                    </header>
                    <div class="table-wrapper" style ="width:100%">
                        <table cellpadding = "0" cellspacing = "0">
                            <tr>
                                <td><img class = "rounded-img-dark" height = "125px" src = "photos/asano.jpg"><br><a href = "https://yukimasano.github.io/">Yuki M Asano<br> University of Amsterdam</a></td>
                                <td><img class = "rounded-img-dark" height = "125px" src = "photos/tengda.jpg"><br><a href = "https://www.robots.ox.ac.uk/~htd">Tengda Han<br>University of Oxford</td>
                                <td><img class="rounded-img-dark" height="125px" src="photos/caron.jpg"><br><a href="https://scholar.google.com/citations?user=eiB0s-kAAAAJ&hl=fr">Mathilde Caron<br>Google</a></td>
                                <td><img class = "rounded-img-dark" height = "125px" src = "photos/phillip.jpeg"><br><a href = "http://web.mit.edu/phillipi/">Phillip Isola<br>MIT</a></td>
                                <td><img class="rounded-img-dark" height="125px" src="photos/serge.jpeg"><br><a href="https://www.belongielab.org/">Serge Belongie<br>University of Copenhagen</a></td>
                            </tr>
                        </table>
                    </div>
                </section>
            </div>

                 
            <!-- Footer -->
            <footer id="footer">
                <p class = "copyright" style="font-size:medium">
                    Website design adapted from <a href = "http://sslwin.org">sslwin.org</a> and <a href = "http://sightsound.org">sightsound.org</a>
                    and based on a template from <a href="https://html5up.net">HTML5 UP</a>.
                </p>
            </footer>
			</div>

		<!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/jquery.scrollex.min.js"></script>
        <script src="assets/js/jquery.scrolly.min.js"></script>
        <script src="assets/js/skel.min.js"></script>
        <script src="assets/js/util.js"></script>
        <!--[if lte IE 8]>
        <script src="assets/js/ie/respond.min.js"></script><![endif]-->
        <!-- <script src="assets/js/main.js"></script> -->
    <script>
      function convertDateAndTimezone(timeStr) {
        var newDate = new Date("2020-08-28T"+timeStr+":00.000Z");
        return newDate.toString();   
      }
      
	    function convertTime(timeStr) {
        var newDate = new Date("2020-08-28T"+timeStr+":00.000Z");
        return newDate.toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'});   
      }
      
      function convertTimeRange(rangeStr) {
        var s = rangeStr.split('-')
        return convertTime(s[0]) + ' - ' + convertTime(s[1]);   
      }
      
      $( document ).ready(function() {
        $( ".convertTime" ).each(function( index ) {
          $(this).text(convertTime($(this).text()));
        });

        $( ".convertDate" ).each(function( index ) {
          $(this).text(convertDateAndTimezone($(this).text()));
        });

        $( ".convertTimeRange" ).each(function( index ) {
          $(this).text(convertTimeRange($(this).text()));
        });
        
        $( ".collapsible" ).click(function() {
            $(this).toggleClass("active");
            $(this).next().toggle();
        });
      });
	  </script>
	</body>
</html>
